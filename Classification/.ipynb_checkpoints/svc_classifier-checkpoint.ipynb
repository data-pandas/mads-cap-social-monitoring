{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classifier - SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn.externals as extjoblib\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.read_csv('cleaned_non-vectorized_data.csv')\n",
    "df_proc = df_proc[~df_proc['Text'].isna()]\n",
    "X = df_proc['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprised</th>\n",
       "      <th>sad</th>\n",
       "      <th>fear</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>come mert â€™ today let u take care lunch enjoy ...</td>\n",
       "      <td>['come', 'mert', 'â€™', 'today', 'let', 'u', 'ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nxt gt lay 20 staff tech 's latest cutback rb_...</td>\n",
       "      <td>['nxt', 'gt', 'lay', '20', 'staff', 'tech', \"'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>layoff 20 workforce 100 employee sf bay area</td>\n",
       "      <td>['layoff', '20', 'workforce', '100', 'employee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today â€™ lunch special smoked pork sausage onio...</td>\n",
       "      <td>['today', 'â€™', 'lunch', 'special', 'smoked', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>come mert â€™ today grab salmon cake two home co...</td>\n",
       "      <td>['come', 'mert', 'â€™', 'today', 'grab', 'salmon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>traik01 cdc people warmed u 2 year ago .... sa...</td>\n",
       "      <td>['traik01', 'cdc', 'people', 'warmed', 'u', '2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>sorry â€™ promo code share lately ðŸ˜­ promos autom...</td>\n",
       "      <td>['sorry', 'â€™', 'promo', 'code', 'share', 'late...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>poor lad</td>\n",
       "      <td>['poor', 'lad']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>one day able bill order tmobile bill sadly tod...</td>\n",
       "      <td>['one', 'day', 'able', 'bill', 'order', 'tmobi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>someone please bless dinner ðŸ¥¹</td>\n",
       "      <td>['someone', 'please', 'bless', 'dinner', '\\U00...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9226 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     come mert â€™ today let u take care lunch enjoy ...   \n",
       "1     nxt gt lay 20 staff tech 's latest cutback rb_...   \n",
       "2          layoff 20 workforce 100 employee sf bay area   \n",
       "3     today â€™ lunch special smoked pork sausage onio...   \n",
       "4     come mert â€™ today grab salmon cake two home co...   \n",
       "...                                                 ...   \n",
       "9277  traik01 cdc people warmed u 2 year ago .... sa...   \n",
       "9278  sorry â€™ promo code share lately ðŸ˜­ promos autom...   \n",
       "9279                                           poor lad   \n",
       "9280  one day able bill order tmobile bill sadly tod...   \n",
       "9281                      someone please bless dinner ðŸ¥¹   \n",
       "\n",
       "                                                 tokens  disgust  joy  anger  \\\n",
       "0     ['come', 'mert', 'â€™', 'today', 'let', 'u', 'ta...        0    0      0   \n",
       "1     ['nxt', 'gt', 'lay', '20', 'staff', 'tech', \"'...        0    0      0   \n",
       "2     ['layoff', '20', 'workforce', '100', 'employee...        0    0      0   \n",
       "3     ['today', 'â€™', 'lunch', 'special', 'smoked', '...        0    0      0   \n",
       "4     ['come', 'mert', 'â€™', 'today', 'grab', 'salmon...        0    0      0   \n",
       "...                                                 ...      ...  ...    ...   \n",
       "9277  ['traik01', 'cdc', 'people', 'warmed', 'u', '2...        0    0      0   \n",
       "9278  ['sorry', 'â€™', 'promo', 'code', 'share', 'late...        0    0      0   \n",
       "9279                                    ['poor', 'lad']        0    0      0   \n",
       "9280  ['one', 'day', 'able', 'bill', 'order', 'tmobi...        0    0      0   \n",
       "9281  ['someone', 'please', 'bless', 'dinner', '\\U00...        0    0      0   \n",
       "\n",
       "      surprised  sad  fear  neutral  \n",
       "0             0    0     0        1  \n",
       "1             0    0     0        1  \n",
       "2             0    0     0        1  \n",
       "3             0    0     0        1  \n",
       "4             0    0     0        1  \n",
       "...         ...  ...   ...      ...  \n",
       "9277          0    1     0        0  \n",
       "9278          0    1     0        0  \n",
       "9279          0    1     0        0  \n",
       "9280          0    1     0        0  \n",
       "9281          0    1     0        0  \n",
       "\n",
       "[9226 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_proc.loc[:,['disgust', 'joy', 'anger', 'surprised', 'sad', 'fear', 'neutral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprised</th>\n",
       "      <th>sad</th>\n",
       "      <th>fear</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9226 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      disgust  joy  anger  surprised  sad  fear  neutral\n",
       "0           0    0      0          0    0     0        1\n",
       "1           0    0      0          0    0     0        1\n",
       "2           0    0      0          0    0     0        1\n",
       "3           0    0      0          0    0     0        1\n",
       "4           0    0      0          0    0     0        1\n",
       "...       ...  ...    ...        ...  ...   ...      ...\n",
       "9277        0    0      0          0    1     0        0\n",
       "9278        0    0      0          0    1     0        0\n",
       "9279        0    0      0          0    1     0        0\n",
       "9280        0    0      0          0    1     0        0\n",
       "9281        0    0      0          0    1     0        0\n",
       "\n",
       "[9226 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-Of-Words Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                             stop_words='english', \n",
    "                             ngram_range=(1,2),\n",
    "                             max_df=0.75,\n",
    "                             min_df=50,\n",
    "                             norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('multi-classifier', multi_out_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vectorizer', 'multi-classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__norm', 'vectorizer__preprocessor', 'vectorizer__smooth_idf', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__sublinear_tf', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__use_idf', 'vectorizer__vocabulary', 'multi-classifier__estimator__C', 'multi-classifier__estimator__class_weight', 'multi-classifier__estimator__dual', 'multi-classifier__estimator__fit_intercept', 'multi-classifier__estimator__intercept_scaling', 'multi-classifier__estimator__loss', 'multi-classifier__estimator__max_iter', 'multi-classifier__estimator__multi_class', 'multi-classifier__estimator__penalty', 'multi-classifier__estimator__random_state', 'multi-classifier__estimator__tol', 'multi-classifier__estimator__verbose', 'multi-classifier__estimator', 'multi-classifier__n_jobs'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"vectorizer__max_df\": (0.5, 0.75, 1.0),\n",
    "    \"vectorizer__min_df\": (50, 100, 200),\n",
    "    \"vectorizer__ngram_range\": ((1, 1), (1, 2)), \n",
    "    \"vectorizer__norm\": ('l1', 'l2'),\n",
    "    \"multi-classifier__estimator__multi_class\": ('ovr', 'crammer_singer'),\n",
    "    'multi-classifier__estimator__C': (0.1, 0.5, 1.0), \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(max_df=0.75, min_df=50,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        norm='l1',\n",
       "                                                        stop_words='english')),\n",
       "                                       ('multi-classifier',\n",
       "                                        MultiOutputClassifier(estimator=LinearSVC(random_state=42)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'multi-classifier__estimator__C': (0.1, 0.5, 1.0),\n",
       "                         'multi-classifier__estimator__multi_class': ('ovr',\n",
       "                                                                      'crammer_singer'),\n",
       "                         'vectorizer__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vectorizer__min_df': (50, 100, 200),\n",
       "                         'vectorizer__ngram_range': ((1, 1), (1, 2)),\n",
       "                         'vectorizer__norm': ('l1', 'l2')},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   TfidfVectorizer(max_df=0.5, min_df=50, ngram_range=(1, 2), stop_words='english')),\n",
       "  ('multi-classifier',\n",
       "   MultiOutputClassifier(estimator=LinearSVC(random_state=42)))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': TfidfVectorizer(max_df=0.5, min_df=50, ngram_range=(1, 2), stop_words='english'),\n",
       " 'multi-classifier': MultiOutputClassifier(estimator=LinearSVC(random_state=42)),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.float64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 0.5,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 50,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__norm': 'l2',\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__smooth_idf': True,\n",
       " 'vectorizer__stop_words': 'english',\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__sublinear_tf': False,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__use_idf': True,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'multi-classifier__estimator__C': 1.0,\n",
       " 'multi-classifier__estimator__class_weight': None,\n",
       " 'multi-classifier__estimator__dual': True,\n",
       " 'multi-classifier__estimator__fit_intercept': True,\n",
       " 'multi-classifier__estimator__intercept_scaling': 1,\n",
       " 'multi-classifier__estimator__loss': 'squared_hinge',\n",
       " 'multi-classifier__estimator__max_iter': 1000,\n",
       " 'multi-classifier__estimator__multi_class': 'ovr',\n",
       " 'multi-classifier__estimator__penalty': 'l2',\n",
       " 'multi-classifier__estimator__random_state': 42,\n",
       " 'multi-classifier__estimator__tol': 0.0001,\n",
       " 'multi-classifier__estimator__verbose': 0,\n",
       " 'multi-classifier__estimator': LinearSVC(random_state=42),\n",
       " 'multi-classifier__n_jobs': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "def print_classification_scores(y_test, pred):\n",
    "    print('Accuracy Score:',accuracy_score(y_test, pred))\n",
    "    print('Precision Score:',precision_score(y_test, pred, average='micro'))\n",
    "    print('Recall Score:',recall_score(y_test, pred, average='micro'))\n",
    "    print('F1 Score:',f1_score(y_test, pred, average='micro'))\n",
    "    print('AUC Score:',roc_auc_score(y_test, pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.46045503791982667\n",
      "Precision Score: 0.6736507936507936\n",
      "Recall Score: 0.496026180458158\n",
      "F1 Score: 0.5713516424340334\n",
      "AUC Score: 0.7241792777464675\n"
     ]
    }
   ],
   "source": [
    "pred = grid_search.predict(X_test)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import nltk\n",
    "import gensim.downloader\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/m.nguyen.2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9226/9226 [00:00<00:00, 117179.48it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_reviews = []\n",
    "\n",
    "for review in tqdm(X):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    all_tokenized_reviews.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Word2Vec(sentences=all_tokenized_reviews, vector_size=100, \n",
    "                      window=2, min_count=100, workers=4, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(\"word2vec_tweets.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_kv = full_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews = gensim.downloader.load('word2vec-google-news-300')\n",
    "glove_small = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "glove_big = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_texts, word_vectors): \n",
    "    #HINT: Create an empty list to hold your results \n",
    "        #HINT:Iterate through each item in tokenized_text\n",
    "            #HINT:Create a list that contains current item(s) if found in word_vectors\n",
    "            #HINT:if the length of this list is greater than zero:\n",
    "                #HINT:We set this as a feature, this is done by using numpyâ€™s mean function and append it to our results list \n",
    "            #HINT:Otherwise: create a vector of numpy zeros using word_vectors.vector_size as the parameter and append it to the results list\n",
    "    #HINT:Return the results list as a numpy array (data type)\n",
    "\n",
    "    res = []\n",
    "    for token in tokenized_texts:\n",
    "        items_in_vocab = [item for item in token if item in word_vectors]\n",
    "        if len(items_in_vocab) > 0:\n",
    "            res.append(np.mean(word_vectors[items_in_vocab], axis=0))\n",
    "        else:\n",
    "            res.append(np.zeros(word_vectors.vector_size))\n",
    "    return np.array(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7380/7380 [00:04<00:00, 1771.48it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_items = []\n",
    "for review in tqdm(X_train):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_train_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1846/1846 [00:00<00:00, 126316.32it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_items = []\n",
    "for review in tqdm(X_test):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_test_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LinearSVC(random_state=42))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.2746478873239437\n",
      "Precision Score: 0.6702014846235419\n",
      "Recall Score: 0.2954651706404862\n",
      "F1 Score: 0.4101232965606749\n",
      "AUC Score: 0.6333117376897136\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat the same process with the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LinearSVC(random_state=42))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.44257854821235104\n",
      "Precision Score: 0.6970284237726099\n",
      "Recall Score: 0.5044413277232351\n",
      "F1 Score: 0.5852997016544617\n",
      "AUC Score: 0.7304734692033592\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LinearSVC(random_state=42))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.38353196099674974\n",
      "Precision Score: 0.6579898770788142\n",
      "Recall Score: 0.4254324450677887\n",
      "F1 Score: 0.5167518455423056\n",
      "AUC Score: 0.6907835507356934\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=LinearSVC(random_state=42))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LinearSVC(random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.43932827735644636\n",
      "Precision Score: 0.680973734785394\n",
      "Recall Score: 0.49696119682094436\n",
      "F1 Score: 0.5745945945945946\n",
      "AUC Score: 0.7253886944876307\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
