{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfd8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn.externals as extjoblib\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#from google.cloud import storage\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b201b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d67b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.read_csv('cleaned_non-vectorized_data.csv')\n",
    "df_proc = df_proc[~df_proc['Text'].isna()]\n",
    "X = df_proc['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9ab12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprised</th>\n",
       "      <th>sad</th>\n",
       "      <th>fear</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>come mert â€™ today let u take care lunch enjoy ...</td>\n",
       "      <td>['come', 'mert', 'â€™', 'today', 'let', 'u', 'ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nxt gt lay 20 staff tech 's latest cutback rb_...</td>\n",
       "      <td>['nxt', 'gt', 'lay', '20', 'staff', 'tech', \"'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>layoff 20 workforce 100 employee sf bay area</td>\n",
       "      <td>['layoff', '20', 'workforce', '100', 'employee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today â€™ lunch special smoked pork sausage onio...</td>\n",
       "      <td>['today', 'â€™', 'lunch', 'special', 'smoked', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>come mert â€™ today grab salmon cake two home co...</td>\n",
       "      <td>['come', 'mert', 'â€™', 'today', 'grab', 'salmon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>traik01 cdc people warmed u 2 year ago .... sa...</td>\n",
       "      <td>['traik01', 'cdc', 'people', 'warmed', 'u', '2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>sorry â€™ promo code share lately ðŸ˜­ promos autom...</td>\n",
       "      <td>['sorry', 'â€™', 'promo', 'code', 'share', 'late...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>poor lad</td>\n",
       "      <td>['poor', 'lad']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>one day able bill order tmobile bill sadly tod...</td>\n",
       "      <td>['one', 'day', 'able', 'bill', 'order', 'tmobi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>someone please bless dinner ðŸ¥¹</td>\n",
       "      <td>['someone', 'please', 'bless', 'dinner', '\\U00...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9226 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     come mert â€™ today let u take care lunch enjoy ...   \n",
       "1     nxt gt lay 20 staff tech 's latest cutback rb_...   \n",
       "2          layoff 20 workforce 100 employee sf bay area   \n",
       "3     today â€™ lunch special smoked pork sausage onio...   \n",
       "4     come mert â€™ today grab salmon cake two home co...   \n",
       "...                                                 ...   \n",
       "9277  traik01 cdc people warmed u 2 year ago .... sa...   \n",
       "9278  sorry â€™ promo code share lately ðŸ˜­ promos autom...   \n",
       "9279                                           poor lad   \n",
       "9280  one day able bill order tmobile bill sadly tod...   \n",
       "9281                      someone please bless dinner ðŸ¥¹   \n",
       "\n",
       "                                                 tokens  disgust  joy  anger  \\\n",
       "0     ['come', 'mert', 'â€™', 'today', 'let', 'u', 'ta...        0    0      0   \n",
       "1     ['nxt', 'gt', 'lay', '20', 'staff', 'tech', \"'...        0    0      0   \n",
       "2     ['layoff', '20', 'workforce', '100', 'employee...        0    0      0   \n",
       "3     ['today', 'â€™', 'lunch', 'special', 'smoked', '...        0    0      0   \n",
       "4     ['come', 'mert', 'â€™', 'today', 'grab', 'salmon...        0    0      0   \n",
       "...                                                 ...      ...  ...    ...   \n",
       "9277  ['traik01', 'cdc', 'people', 'warmed', 'u', '2...        0    0      0   \n",
       "9278  ['sorry', 'â€™', 'promo', 'code', 'share', 'late...        0    0      0   \n",
       "9279                                    ['poor', 'lad']        0    0      0   \n",
       "9280  ['one', 'day', 'able', 'bill', 'order', 'tmobi...        0    0      0   \n",
       "9281  ['someone', 'please', 'bless', 'dinner', '\\U00...        0    0      0   \n",
       "\n",
       "      surprised  sad  fear  neutral  \n",
       "0             0    0     0        1  \n",
       "1             0    0     0        1  \n",
       "2             0    0     0        1  \n",
       "3             0    0     0        1  \n",
       "4             0    0     0        1  \n",
       "...         ...  ...   ...      ...  \n",
       "9277          0    1     0        0  \n",
       "9278          0    1     0        0  \n",
       "9279          0    1     0        0  \n",
       "9280          0    1     0        0  \n",
       "9281          0    1     0        0  \n",
       "\n",
       "[9226 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2635bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_proc.loc[:,['disgust', 'joy', 'anger', 'surprised', 'sad', 'fear', 'neutral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458989e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprised</th>\n",
       "      <th>sad</th>\n",
       "      <th>fear</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9226 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      disgust  joy  anger  surprised  sad  fear  neutral\n",
       "0           0    0      0          0    0     0        1\n",
       "1           0    0      0          0    0     0        1\n",
       "2           0    0      0          0    0     0        1\n",
       "3           0    0      0          0    0     0        1\n",
       "4           0    0      0          0    0     0        1\n",
       "...       ...  ...    ...        ...  ...   ...      ...\n",
       "9277        0    0      0          0    1     0        0\n",
       "9278        0    0      0          0    1     0        0\n",
       "9279        0    0      0          0    1     0        0\n",
       "9280        0    0      0          0    1     0        0\n",
       "9281        0    0      0          0    1     0        0\n",
       "\n",
       "[9226 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c29e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80432a35",
   "metadata": {},
   "source": [
    "### Preprocessing - BOW Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "827c76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                             stop_words='english', \n",
    "                             ngram_range=(1,2),\n",
    "                             max_df=0.75,\n",
    "                             min_df=50,\n",
    "                             norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f1f572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f359b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the features and fit them to the classifier\n",
    "# multi_out_clf.fit(transformer.fit_transform(vectorizer.fit_transform(X_train)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea12f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('multi-classifier', multi_out_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9242cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vectorizer', 'multi-classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__norm', 'vectorizer__preprocessor', 'vectorizer__smooth_idf', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__sublinear_tf', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__use_idf', 'vectorizer__vocabulary', 'multi-classifier__estimator__bootstrap', 'multi-classifier__estimator__ccp_alpha', 'multi-classifier__estimator__class_weight', 'multi-classifier__estimator__criterion', 'multi-classifier__estimator__max_depth', 'multi-classifier__estimator__max_features', 'multi-classifier__estimator__max_leaf_nodes', 'multi-classifier__estimator__max_samples', 'multi-classifier__estimator__min_impurity_decrease', 'multi-classifier__estimator__min_impurity_split', 'multi-classifier__estimator__min_samples_leaf', 'multi-classifier__estimator__min_samples_split', 'multi-classifier__estimator__min_weight_fraction_leaf', 'multi-classifier__estimator__n_estimators', 'multi-classifier__estimator__n_jobs', 'multi-classifier__estimator__oob_score', 'multi-classifier__estimator__random_state', 'multi-classifier__estimator__verbose', 'multi-classifier__estimator__warm_start', 'multi-classifier__estimator', 'multi-classifier__n_jobs'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64fecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"vectorizer__max_df\": (0.5, 0.75, 1.0),\n",
    "    \"vectorizer__min_df\": (50, 100, 200),\n",
    "    \"vectorizer__ngram_range\": ((1, 1), (1, 2), (1, 3)), \n",
    "    \"vectorizer__norm\": ('l1', 'l2'),\n",
    "    \"multi-classifier__estimator__criterion\": ('gini', 'log_loss'),\n",
    "    'multi-classifier__estimator__n_estimators': (10, 25, 100), \n",
    "    'multi-classifier__estimator__max_depth': (10, 50, None), \n",
    "    'multi-classifier__estimator__bootstrap': (True, False),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb898764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1944 candidates, totalling 9720 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(max_df=0.75, min_df=50,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        norm='l1',\n",
       "                                                        stop_words='english')),\n",
       "                                       ('multi-classifier',\n",
       "                                        MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'multi-classifier__estimator__bootstrap': (True,\n",
       "                                                                    False),\n",
       "                         'multi-classifier__estimator__criterion': ('gini',\n",
       "                                                                    'log_loss'),\n",
       "                         'multi-classifier__estimator__max_depth': (10, 50,\n",
       "                                                                    None),\n",
       "                         'multi-classifier__estimator__n_estimators': (10, 25,\n",
       "                                                                       100),\n",
       "                         'vectorizer__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vectorizer__min_df': (50, 100, 200),\n",
       "                         'vectorizer__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
       "                         'vectorizer__norm': ('l1', 'l2')},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7d0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bea35d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   TfidfVectorizer(max_df=0.5, min_df=50, stop_words='english')),\n",
       "  ('multi-classifier',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=False,\n",
       "                                                          max_depth=50,\n",
       "                                                          random_state=42)))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': TfidfVectorizer(max_df=0.5, min_df=50, stop_words='english'),\n",
       " 'multi-classifier': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=False,\n",
       "                                                        max_depth=50,\n",
       "                                                        random_state=42)),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.float64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 0.5,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 50,\n",
       " 'vectorizer__ngram_range': (1, 1),\n",
       " 'vectorizer__norm': 'l2',\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__smooth_idf': True,\n",
       " 'vectorizer__stop_words': 'english',\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__sublinear_tf': False,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__use_idf': True,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'multi-classifier__estimator__bootstrap': False,\n",
       " 'multi-classifier__estimator__ccp_alpha': 0.0,\n",
       " 'multi-classifier__estimator__class_weight': None,\n",
       " 'multi-classifier__estimator__criterion': 'gini',\n",
       " 'multi-classifier__estimator__max_depth': 50,\n",
       " 'multi-classifier__estimator__max_features': 'auto',\n",
       " 'multi-classifier__estimator__max_leaf_nodes': None,\n",
       " 'multi-classifier__estimator__max_samples': None,\n",
       " 'multi-classifier__estimator__min_impurity_decrease': 0.0,\n",
       " 'multi-classifier__estimator__min_impurity_split': None,\n",
       " 'multi-classifier__estimator__min_samples_leaf': 1,\n",
       " 'multi-classifier__estimator__min_samples_split': 2,\n",
       " 'multi-classifier__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'multi-classifier__estimator__n_estimators': 100,\n",
       " 'multi-classifier__estimator__n_jobs': None,\n",
       " 'multi-classifier__estimator__oob_score': False,\n",
       " 'multi-classifier__estimator__random_state': 42,\n",
       " 'multi-classifier__estimator__verbose': 0,\n",
       " 'multi-classifier__estimator__warm_start': False,\n",
       " 'multi-classifier__estimator': RandomForestClassifier(bootstrap=False, max_depth=50, random_state=42),\n",
       " 'multi-classifier__n_jobs': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a11b6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "def print_classification_scores(y_test, pred):\n",
    "    print('Accuracy Score:',accuracy_score(y_test, pred))\n",
    "    print('Precision Score:',precision_score(y_test, pred, average='micro'))\n",
    "    print('Recall Score:',recall_score(y_test, pred, average='micro'))\n",
    "    print('F1 Score:',f1_score(y_test, pred, average='micro'))\n",
    "    print('AUC Score:',roc_auc_score(y_test, pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78fc5dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.4962080173347779\n",
      "Precision Score: 0.6794117647058824\n",
      "Recall Score: 0.5399719495091164\n",
      "F1 Score: 0.6017191977077364\n",
      "AUC Score: 0.7447147144373923\n"
     ]
    }
   ],
   "source": [
    "pred = grid_search.predict(X_test)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f42eaa",
   "metadata": {},
   "source": [
    "### Preprocess - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36e29502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (6.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fde587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import nltk\n",
    "import gensim.downloader\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "487d96d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/m.nguyen.2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33f5317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bc635f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9226/9226 [00:00<00:00, 69623.33it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_reviews = []\n",
    "\n",
    "for review in tqdm(X):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    all_tokenized_reviews.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f83b413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Word2Vec(sentences=all_tokenized_reviews, vector_size=100, \n",
    "                      window=2, min_count=100, workers=4, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e95bda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(\"word2vec_tweets.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2c7eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_kv = full_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96916aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnews = gensim.downloader.load('word2vec-google-news-300')\n",
    "glove_small = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "glove_big = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc03d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_texts, word_vectors): \n",
    "    #HINT: Create an empty list to hold your results \n",
    "        #HINT:Iterate through each item in tokenized_text\n",
    "            #HINT:Create a list that contains current item(s) if found in word_vectors\n",
    "            #HINT:if the length of this list is greater than zero:\n",
    "                #HINT:We set this as a feature, this is done by using numpyâ€™s mean function and append it to our results list \n",
    "            #HINT:Otherwise: create a vector of numpy zeros using word_vectors.vector_size as the parameter and append it to the results list\n",
    "    #HINT:Return the results list as a numpy array (data type)\n",
    "\n",
    "    res = []\n",
    "    for token in tokenized_texts:\n",
    "        items_in_vocab = [item for item in token if item in word_vectors]\n",
    "        if len(items_in_vocab) > 0:\n",
    "            res.append(np.mean(word_vectors[items_in_vocab], axis=0))\n",
    "        else:\n",
    "            res.append(np.zeros(word_vectors.vector_size))\n",
    "    return np.array(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ee5e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7380/7380 [00:00<00:00, 17608.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_items = []\n",
    "for review in tqdm(X_train):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_train_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3150397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1846/1846 [00:00<00:00, 146022.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_items = []\n",
    "for review in tqdm(X_test):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_test_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa6bc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac0fc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0174be51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=50,\n",
       "                                                       random_state=42))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=50, random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb4c5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.4528710725893825\n",
      "Precision Score: 0.6940397350993377\n",
      "Recall Score: 0.4899485741000468\n",
      "F1 Score: 0.5744039462866538\n",
      "AUC Score: 0.7235516773866643\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5f8a1",
   "metadata": {},
   "source": [
    "Now we can repeat the same process with the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b971857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "224edc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cd3b629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=50,\n",
       "                                                       random_state=42))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=50, random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e3e85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.4609967497291441\n",
      "Precision Score: 0.7644476956839795\n",
      "Recall Score: 0.4885460495558672\n",
      "F1 Score: 0.5961209355390759\n",
      "AUC Score: 0.7293421150125623\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22742332",
   "metadata": {},
   "source": [
    "Glove_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc2c215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0771d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47ef1f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=RandomForestClassifier(random_state=42))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b45f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.44799566630552545\n",
      "Precision Score: 0.7477941176470588\n",
      "Recall Score: 0.47545582047685836\n",
      "F1 Score: 0.5813089454129751\n",
      "AUC Score: 0.7218232454883596\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16381b",
   "metadata": {},
   "source": [
    "Glove_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb9ecb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1288633",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1722e06",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTreeClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-3608a7be1a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmulti_out_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiOutputClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmulti_out_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_wp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTreeClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=50, random_state=RANDOM_SEED)\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a4c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212b887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
