{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfd8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn.externals as extjoblib\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#from google.cloud import storage\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b201b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d67b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.read_csv('cleaned_non-vectorized_data.csv')\n",
    "df_proc = df_proc[~df_proc['Text'].isna()]\n",
    "X = df_proc['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9ab12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprised</th>\n",
       "      <th>sad</th>\n",
       "      <th>fear</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>come mert â€™ today let u take care lunch enjoy ...</td>\n",
       "      <td>['come', 'mert', 'â€™', 'today', 'let', 'u', 'ta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nxt gt lay 20 staff tech 's latest cutback rb_...</td>\n",
       "      <td>['nxt', 'gt', 'lay', '20', 'staff', 'tech', \"'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>layoff 20 workforce 100 employee sf bay area h...</td>\n",
       "      <td>['layoff', '20', 'workforce', '100', 'employee...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>today â€™ lunch special smoked pork sausage onio...</td>\n",
       "      <td>['today', 'â€™', 'lunch', 'special', 'smoked', '...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>come mert â€™ today grab salmon cake two home co...</td>\n",
       "      <td>['come', 'mert', 'â€™', 'today', 'grab', 'salmon...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>traik01 cdc people warmed u 2 year ago .... sa...</td>\n",
       "      <td>['traik01', 'cdc', 'people', 'warmed', 'u', '2...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>sorry â€™ promo code share lately ðŸ˜­ promos autom...</td>\n",
       "      <td>['sorry', 'â€™', 'promo', 'code', 'share', 'late...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>poor lad http //t.co/36o565zsc3</td>\n",
       "      <td>['poor', 'lad', 'http', '//t.co/36o565zsc3']</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>one day able bill order tmobile bill sadly tod...</td>\n",
       "      <td>['one', 'day', 'able', 'bill', 'order', 'tmobi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>someone please bless dinner ðŸ¥¹</td>\n",
       "      <td>['someone', 'please', 'bless', 'dinner', '\\U00...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9279 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  \\\n",
       "0     come mert â€™ today let u take care lunch enjoy ...   \n",
       "1     nxt gt lay 20 staff tech 's latest cutback rb_...   \n",
       "2     layoff 20 workforce 100 employee sf bay area h...   \n",
       "3     today â€™ lunch special smoked pork sausage onio...   \n",
       "4     come mert â€™ today grab salmon cake two home co...   \n",
       "...                                                 ...   \n",
       "9277  traik01 cdc people warmed u 2 year ago .... sa...   \n",
       "9278  sorry â€™ promo code share lately ðŸ˜­ promos autom...   \n",
       "9279                    poor lad http //t.co/36o565zsc3   \n",
       "9280  one day able bill order tmobile bill sadly tod...   \n",
       "9281                      someone please bless dinner ðŸ¥¹   \n",
       "\n",
       "                                                 tokens  disgust  joy  anger  \\\n",
       "0     ['come', 'mert', 'â€™', 'today', 'let', 'u', 'ta...        0    0      0   \n",
       "1     ['nxt', 'gt', 'lay', '20', 'staff', 'tech', \"'...        0    0      0   \n",
       "2     ['layoff', '20', 'workforce', '100', 'employee...        0    0      0   \n",
       "3     ['today', 'â€™', 'lunch', 'special', 'smoked', '...        0    0      0   \n",
       "4     ['come', 'mert', 'â€™', 'today', 'grab', 'salmon...        0    0      0   \n",
       "...                                                 ...      ...  ...    ...   \n",
       "9277  ['traik01', 'cdc', 'people', 'warmed', 'u', '2...        0    0      0   \n",
       "9278  ['sorry', 'â€™', 'promo', 'code', 'share', 'late...        0    0      0   \n",
       "9279       ['poor', 'lad', 'http', '//t.co/36o565zsc3']        0    0      0   \n",
       "9280  ['one', 'day', 'able', 'bill', 'order', 'tmobi...        0    0      0   \n",
       "9281  ['someone', 'please', 'bless', 'dinner', '\\U00...        0    0      0   \n",
       "\n",
       "      surprised  sad  fear  neutral  \n",
       "0             0    0     0        1  \n",
       "1             0    0     0        1  \n",
       "2             0    0     0        1  \n",
       "3             0    0     0        1  \n",
       "4             0    0     0        1  \n",
       "...         ...  ...   ...      ...  \n",
       "9277          0    1     0        0  \n",
       "9278          0    1     0        0  \n",
       "9279          0    1     0        0  \n",
       "9280          0    1     0        0  \n",
       "9281          0    1     0        0  \n",
       "\n",
       "[9279 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2635bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_proc.loc[:,['disgust', 'joy', 'anger', 'surprised', 'sad', 'fear', 'neutral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458989e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disgust</th>\n",
       "      <th>joy</th>\n",
       "      <th>anger</th>\n",
       "      <th>surprised</th>\n",
       "      <th>sad</th>\n",
       "      <th>fear</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9279 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      disgust  joy  anger  surprised  sad  fear  neutral\n",
       "0           0    0      0          0    0     0        1\n",
       "1           0    0      0          0    0     0        1\n",
       "2           0    0      0          0    0     0        1\n",
       "3           0    0      0          0    0     0        1\n",
       "4           0    0      0          0    0     0        1\n",
       "...       ...  ...    ...        ...  ...   ...      ...\n",
       "9277        0    0      0          0    1     0        0\n",
       "9278        0    0      0          0    1     0        0\n",
       "9279        0    0      0          0    1     0        0\n",
       "9280        0    0      0          0    1     0        0\n",
       "9281        0    0      0          0    1     0        0\n",
       "\n",
       "[9279 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c29e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80432a35",
   "metadata": {},
   "source": [
    "### Preprocessing - BOW Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "827c76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1,2), min_df=50)\n",
    "transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1f572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=RANDOM_SEED, \n",
    "                             #max_features='log2',\n",
    "                             #class_weight='balanced'\n",
    "                            )\n",
    "multi_out_clf = MultiOutputClassifier(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f359b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the features and fit them to the classifier\n",
    "# multi_out_clf.fit(transformer.fit_transform(vectorizer.fit_transform(X_train)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea12f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('transformer', transformer),\n",
    "    ('multi-classifier', multi_out_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64fecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"vectorizer__max_df\": (0.5, 0.75, 1.0),\n",
    "    'vectorizer__max_features': (None, 5000, 10000, 50000),\n",
    "    \"vectorizer__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'transformer__use_idf': (True, False),\n",
    "    'transformer__norm': ('l1', 'l2'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9242cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'vectorizer', 'transformer', 'multi-classifier', 'vectorizer__analyzer', 'vectorizer__binary', 'vectorizer__decode_error', 'vectorizer__dtype', 'vectorizer__encoding', 'vectorizer__input', 'vectorizer__lowercase', 'vectorizer__max_df', 'vectorizer__max_features', 'vectorizer__min_df', 'vectorizer__ngram_range', 'vectorizer__preprocessor', 'vectorizer__stop_words', 'vectorizer__strip_accents', 'vectorizer__token_pattern', 'vectorizer__tokenizer', 'vectorizer__vocabulary', 'transformer__norm', 'transformer__smooth_idf', 'transformer__sublinear_tf', 'transformer__use_idf', 'multi-classifier__estimator__ccp_alpha', 'multi-classifier__estimator__class_weight', 'multi-classifier__estimator__criterion', 'multi-classifier__estimator__max_depth', 'multi-classifier__estimator__max_features', 'multi-classifier__estimator__max_leaf_nodes', 'multi-classifier__estimator__min_impurity_decrease', 'multi-classifier__estimator__min_impurity_split', 'multi-classifier__estimator__min_samples_leaf', 'multi-classifier__estimator__min_samples_split', 'multi-classifier__estimator__min_weight_fraction_leaf', 'multi-classifier__estimator__random_state', 'multi-classifier__estimator__splitter', 'multi-classifier__estimator', 'multi-classifier__n_jobs'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb898764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        CountVectorizer(min_df=50,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        stop_words='english')),\n",
       "                                       ('transformer', TfidfTransformer()),\n",
       "                                       ('multi-classifier',\n",
       "                                        MultiOutputClassifier(estimator=DecisionTreeClassifier(random_state=42)))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'transformer__norm': ('l1', 'l2'),\n",
       "                         'transformer__use_idf': (True, False),\n",
       "                         'vectorizer__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vectorizer__max_features': (None, 5000, 10000, 50000),\n",
       "                         'vectorizer__ngram_range': ((1, 1), (1, 2))},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f7d0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bea35d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vectorizer',\n",
       "   CountVectorizer(max_df=0.75, min_df=50, ngram_range=(1, 2),\n",
       "                   stop_words='english')),\n",
       "  ('transformer', TfidfTransformer(norm='l1', use_idf=False)),\n",
       "  ('multi-classifier',\n",
       "   MultiOutputClassifier(estimator=DecisionTreeClassifier(random_state=42)))],\n",
       " 'verbose': False,\n",
       " 'vectorizer': CountVectorizer(max_df=0.75, min_df=50, ngram_range=(1, 2),\n",
       "                 stop_words='english'),\n",
       " 'transformer': TfidfTransformer(norm='l1', use_idf=False),\n",
       " 'multi-classifier': MultiOutputClassifier(estimator=DecisionTreeClassifier(random_state=42)),\n",
       " 'vectorizer__analyzer': 'word',\n",
       " 'vectorizer__binary': False,\n",
       " 'vectorizer__decode_error': 'strict',\n",
       " 'vectorizer__dtype': numpy.int64,\n",
       " 'vectorizer__encoding': 'utf-8',\n",
       " 'vectorizer__input': 'content',\n",
       " 'vectorizer__lowercase': True,\n",
       " 'vectorizer__max_df': 0.75,\n",
       " 'vectorizer__max_features': None,\n",
       " 'vectorizer__min_df': 50,\n",
       " 'vectorizer__ngram_range': (1, 2),\n",
       " 'vectorizer__preprocessor': None,\n",
       " 'vectorizer__stop_words': 'english',\n",
       " 'vectorizer__strip_accents': None,\n",
       " 'vectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vectorizer__tokenizer': None,\n",
       " 'vectorizer__vocabulary': None,\n",
       " 'transformer__norm': 'l1',\n",
       " 'transformer__smooth_idf': True,\n",
       " 'transformer__sublinear_tf': False,\n",
       " 'transformer__use_idf': False,\n",
       " 'multi-classifier__estimator__ccp_alpha': 0.0,\n",
       " 'multi-classifier__estimator__class_weight': None,\n",
       " 'multi-classifier__estimator__criterion': 'gini',\n",
       " 'multi-classifier__estimator__max_depth': None,\n",
       " 'multi-classifier__estimator__max_features': None,\n",
       " 'multi-classifier__estimator__max_leaf_nodes': None,\n",
       " 'multi-classifier__estimator__min_impurity_decrease': 0.0,\n",
       " 'multi-classifier__estimator__min_impurity_split': None,\n",
       " 'multi-classifier__estimator__min_samples_leaf': 1,\n",
       " 'multi-classifier__estimator__min_samples_split': 2,\n",
       " 'multi-classifier__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'multi-classifier__estimator__random_state': 42,\n",
       " 'multi-classifier__estimator__splitter': 'best',\n",
       " 'multi-classifier__estimator': DecisionTreeClassifier(random_state=42),\n",
       " 'multi-classifier__n_jobs': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a11b6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "def print_classification_scores(y_test, pred):\n",
    "    print('Accuracy Score:',accuracy_score(y_test, pred))\n",
    "    print('Precision Score:',precision_score(y_test, pred, average='micro'))\n",
    "    print('Recall Score:',recall_score(y_test, pred, average='micro'))\n",
    "    print('F1 Score:',f1_score(y_test, pred, average='micro'))\n",
    "    print('AUC Score:',roc_auc_score(y_test, pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78fc5dc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.4639008620689655\n",
      "Precision Score: 0.624432104997476\n",
      "Recall Score: 0.5834905660377359\n",
      "F1 Score: 0.6032674957327482\n",
      "AUC Score: 0.7575289474780291\n"
     ]
    }
   ],
   "source": [
    "pred = grid_search.predict(X_test)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f42eaa",
   "metadata": {},
   "source": [
    "### Preprocess - Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36e29502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp38-cp38-macosx_10_9_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.0 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.6.2)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58 kB 5.8 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /Users/m.nguyen.2/opt/anaconda3/lib/python3.8/site-packages (from gensim) (1.20.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fde587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import nltk\n",
    "import gensim.downloader\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "487d96d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/m.nguyen.2/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33f5317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc635f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9279/9279 [00:00<00:00, 93061.00it/s]\n"
     ]
    }
   ],
   "source": [
    "all_tokenized_reviews = []\n",
    "\n",
    "for review in tqdm(X):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    all_tokenized_reviews.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f83b413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = Word2Vec(sentences=all_tokenized_reviews, vector_size=100, \n",
    "                      window=2, min_count=100, workers=4, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e95bda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(\"word2vec_tweets.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2c7eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_kv = full_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96916aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "gnews = gensim.downloader.load('word2vec-google-news-300')\n",
    "glove_small = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "glove_big = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc03d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dense_features(tokenized_texts, word_vectors): \n",
    "    #HINT: Create an empty list to hold your results \n",
    "        #HINT:Iterate through each item in tokenized_text\n",
    "            #HINT:Create a list that contains current item(s) if found in word_vectors\n",
    "            #HINT:if the length of this list is greater than zero:\n",
    "                #HINT:We set this as a feature, this is done by using numpyâ€™s mean function and append it to our results list \n",
    "            #HINT:Otherwise: create a vector of numpy zeros using word_vectors.vector_size as the parameter and append it to the results list\n",
    "    #HINT:Return the results list as a numpy array (data type)\n",
    "\n",
    "    res = []\n",
    "    for token in tokenized_texts:\n",
    "        items_in_vocab = [item for item in token if item in word_vectors]\n",
    "        if len(items_in_vocab) > 0:\n",
    "            res.append(np.mean(word_vectors[items_in_vocab], axis=0))\n",
    "        else:\n",
    "            res.append(np.zeros(word_vectors.vector_size))\n",
    "    return np.array(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ee5e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7423/7423 [00:00<00:00, 90438.39it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_items = []\n",
    "for review in tqdm(X_train):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_train_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3150397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1856/1856 [00:00<00:00, 89484.66it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_test_items = []\n",
    "for review in tqdm(X_test):\n",
    "    tokens = [token for token in re.findall(r'\\w+', review) if token not in stop_words]\n",
    "    tokenized_test_items.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa6bc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac0fc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, full_model_kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0174be51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=DecisionTreeClassifier(random_state=42))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=RANDOM_SEED, \n",
    "                             #max_features='log2',\n",
    "                             #class_weight='balanced'\n",
    "                            )\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fb4c5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.41379310344827586\n",
      "Precision Score: 0.5653642064640617\n",
      "Recall Score: 0.5528301886792453\n",
      "F1 Score: 0.5590269496780349\n",
      "AUC Score: 0.7349783761644938\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f5f8a1",
   "metadata": {},
   "source": [
    "Now we can repeat the same process with the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b971857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "224edc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, gnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cd3b629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=DecisionTreeClassifier(random_state=42))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=RANDOM_SEED, \n",
    "                             #max_features='log2',\n",
    "                             #class_weight='balanced'\n",
    "                            )\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e3e85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.37176724137931033\n",
      "Precision Score: 0.5295376712328768\n",
      "Recall Score: 0.5834905660377359\n",
      "F1 Score: 0.5552064631956912\n",
      "AUC Score: 0.7412026045788385\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22742332",
   "metadata": {},
   "source": [
    "Glove_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc2c215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0771d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, glove_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47ef1f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=DecisionTreeClassifier(random_state=42))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=RANDOM_SEED, \n",
    "                             #max_features='log2',\n",
    "                             #class_weight='balanced'\n",
    "                            )\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b45f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.3739224137931034\n",
      "Precision Score: 0.533801580333626\n",
      "Recall Score: 0.5735849056603773\n",
      "F1 Score: 0.5529786266484766\n",
      "AUC Score: 0.7379513932275397\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16381b",
   "metadata": {},
   "source": [
    "Glove_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb9ecb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_wp = generate_dense_features(tokenized_train_items, glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1288633",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_wp = generate_dense_features(tokenized_test_items, glove_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1722e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=DecisionTreeClassifier(random_state=42))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=RANDOM_SEED, \n",
    "                             #max_features='log2',\n",
    "                             #class_weight='balanced'\n",
    "                            )\n",
    "multi_out_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "\n",
    "multi_out_clf.fit(X_train_wp, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a9a4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.36476293103448276\n",
      "Precision Score: 0.5257040679481448\n",
      "Recall Score: 0.5547169811320755\n",
      "F1 Score: 0.539820977736975\n",
      "AUC Score: 0.7285634206616964\n"
     ]
    }
   ],
   "source": [
    "pred = multi_out_clf.predict(X_test_wp)\n",
    "print_classification_scores(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212b887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
